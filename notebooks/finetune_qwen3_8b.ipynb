{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Qwen3-8B HR 도메인 파인튜닝\n",
        "\n",
        "**환경**: Google Colab (T4 15GB / A100 40GB)\n",
        "\n",
        "**목표**:\n",
        "- SQL 생성 품질 향상\n",
        "- 회사 규정 QA 품질 향상\n",
        "\n",
        "**데이터셋**: 200개 (SQL 100 + RAG 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. 환경 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Unsloth 설치 (Colab 최적화)\n",
        "%%capture\n",
        "!pip install unsloth\n",
        "!pip install --no-deps trl peft accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GPU 확인\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. 데이터셋 업로드\n",
        "\n",
        "GitHub에서 직접 다운로드하거나 파일 업로드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 방법 1: GitHub Raw URL에서 다운로드 (repo가 public인 경우)\n",
        "# !wget -O combined_train.json \"https://raw.githubusercontent.com/K-tuna/enterprise-hr-agent/main/data/finetuning/combined_train.json\"\n",
        "\n",
        "# 방법 2: Google Drive에서 마운트\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Drive에 combined_train.json 업로드 후 경로 지정\n",
        "DATASET_PATH = \"/content/drive/MyDrive/enterprise-hr-agent/combined_train.json\"\n",
        "\n",
        "# 방법 3: 직접 업로드\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()  # combined_train.json 선택\n",
        "# DATASET_PATH = \"combined_train.json\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 데이터셋 확인\n",
        "import json\n",
        "\n",
        "with open(DATASET_PATH, 'r', encoding='utf-8') as f:\n",
        "    raw_data = json.load(f)\n",
        "\n",
        "print(f\"총 데이터 수: {len(raw_data)}개\")\n",
        "print(f\"\\n첫 번째 예시:\")\n",
        "print(json.dumps(raw_data[0], ensure_ascii=False, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. 모델 로드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "# 설정\n",
        "max_seq_length = 2048\n",
        "lora_rank = 32  # 8GB에서는 16, 충분하면 32\n",
        "\n",
        "# 모델 로드 (4-bit 양자화)\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/Qwen3-8B\",  # 또는 \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "    max_seq_length=max_seq_length,\n",
        "    load_in_4bit=True,\n",
        "    dtype=None,  # 자동 감지\n",
        ")\n",
        "\n",
        "print(f\"모델 로드 완료: {model.config.model_type}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LoRA 어댑터 설정\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=lora_rank,\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ],\n",
        "    lora_alpha=lora_rank * 2,  # 학습 속도 향상\n",
        "    lora_dropout=0,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=42,\n",
        "    max_seq_length=max_seq_length,\n",
        ")\n",
        "\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"학습 가능 파라미터: {trainable_params:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. 데이터셋 준비"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "from unsloth.chat_templates import get_chat_template, standardize_sharegpt\n",
        "\n",
        "# Tokenizer에 chat template 적용\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template=\"qwen-2.5\",  # Qwen3도 동일\n",
        ")\n",
        "\n",
        "# 포맷팅 함수\n",
        "def formatting_prompts_func(examples):\n",
        "    convos = examples[\"conversations\"]\n",
        "    texts = [\n",
        "        tokenizer.apply_chat_template(\n",
        "            convo, \n",
        "            tokenize=False, \n",
        "            add_generation_prompt=False\n",
        "        ) \n",
        "        for convo in convos\n",
        "    ]\n",
        "    return {\"text\": texts}\n",
        "\n",
        "# Dataset 생성\n",
        "dataset = Dataset.from_list(raw_data)\n",
        "dataset = standardize_sharegpt(dataset)\n",
        "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
        "\n",
        "print(f\"\\n포맷팅된 첫 번째 예시:\")\n",
        "print(dataset[0][\"text\"][:500])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. 학습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dataset_num_proc=2,\n",
        "    packing=True,  # 짧은 시퀀스 패킹으로 효율성 향상\n",
        "    args=TrainingArguments(\n",
        "        output_dir=\"./outputs\",\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "        num_train_epochs=3,\n",
        "        learning_rate=2e-4,\n",
        "        warmup_steps=10,\n",
        "        logging_steps=10,\n",
        "        save_steps=100,\n",
        "        fp16=not is_bfloat16_supported(),\n",
        "        bf16=is_bfloat16_supported(),\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=42,\n",
        "        report_to=\"none\",\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 학습 시작\n",
        "print(\"=\" * 50)\n",
        "print(\"학습 시작...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "trainer_stats = trainer.train()\n",
        "\n",
        "print(\"\\n학습 완료!\")\n",
        "print(f\"총 학습 시간: {trainer_stats.metrics['train_runtime']:.2f}초\")\n",
        "print(f\"최종 Loss: {trainer_stats.metrics['train_loss']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. 테스트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 추론 모드 활성화\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# 테스트 질문\n",
        "test_questions = [\n",
        "    \"개발팀 평균 급여는?\",\n",
        "    \"연차휴가 며칠이야?\",\n",
        "    \"김철수 평가 점수 알려줘\",\n",
        "    \"재택근무 가능해?\",\n",
        "]\n",
        "\n",
        "for question in test_questions:\n",
        "    messages = [{\"role\": \"user\", \"content\": question}]\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(model.device)\n",
        "    \n",
        "    outputs = model.generate(\n",
        "        inputs,\n",
        "        max_new_tokens=256,\n",
        "        temperature=0.1,\n",
        "        do_sample=True,\n",
        "    )\n",
        "    \n",
        "    response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
        "    print(f\"Q: {question}\")\n",
        "    print(f\"A: {response}\")\n",
        "    print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. GGUF 변환 및 저장"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GGUF 변환 (Ollama용)\n",
        "model.save_pretrained_gguf(\n",
        "    \"qwen3-hr\",\n",
        "    tokenizer,\n",
        "    quantization_method=\"q4_k_m\",  # 4-bit 양자화 (품질/크기 균형)\n",
        ")\n",
        "\n",
        "print(\"GGUF 변환 완료!\")\n",
        "!ls -lh qwen3-hr*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Google Drive에 저장\n",
        "import shutil\n",
        "\n",
        "SAVE_DIR = \"/content/drive/MyDrive/enterprise-hr-agent/models\"\n",
        "!mkdir -p \"{SAVE_DIR}\"\n",
        "\n",
        "# GGUF 파일 복사\n",
        "!cp qwen3-hr-q4_k_m.gguf \"{SAVE_DIR}/\"\n",
        "\n",
        "print(f\"\\n저장 완료: {SAVE_DIR}\")\n",
        "!ls -lh \"{SAVE_DIR}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. 로컬 배포 가이드\n",
        "\n",
        "### Google Drive에서 다운로드\n",
        "1. Google Drive에서 `qwen3-hr-q4_k_m.gguf` 파일 다운로드\n",
        "2. `models/` 디렉토리에 저장\n",
        "\n",
        "### Ollama 등록\n",
        "```bash\n",
        "# Modelfile 생성\n",
        "echo 'FROM ./qwen3-hr-q4_k_m.gguf\n",
        "TEMPLATE \"\"\"{{ if .System }}<|im_start|>system\n",
        "{{ .System }}<|im_end|>\n",
        "{{ end }}<|im_start|>user\n",
        "{{ .Prompt }}<|im_end|>\n",
        "<|im_start|>assistant\n",
        "\"\"\"' > Modelfile\n",
        "\n",
        "# Ollama에 등록\n",
        "ollama create qwen3-hr -f Modelfile\n",
        "\n",
        "# 테스트\n",
        "ollama run qwen3-hr \"개발팀 평균 급여 SQL 작성해줘\"\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"파인튜닝 완료!\")\n",
        "print(\"\\n다음 단계:\")\n",
        "print(\"1. Google Drive에서 GGUF 파일 다운로드\")\n",
        "print(\"2. 로컬에서 Ollama에 등록\")\n",
        "print(\"3. 기존 qwen3:8b와 성능 비교\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
